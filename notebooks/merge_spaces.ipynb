{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from la.utils.utils import MyDatasetDict\n",
    "\n",
    "try:\n",
    "    # be ready for 3.10 when it drops\n",
    "    from enum import StrEnum\n",
    "except ImportError:\n",
    "    from backports.strenum import StrEnum\n",
    "from pytorch_lightning import seed_everything\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import timm\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from typing import Sequence, List\n",
    "from PIL.Image import Image\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "from timm.data import resolve_data_config\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "from timm.data import create_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR: Path = PROJECT_ROOT / \"data\" / \"cifar100_tasks\"\n",
    "dataset: MyDatasetDict = MyDatasetDict.load_from_disk(dataset_dict_path=str(DATASET_DIR))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that all the tasks only have the desired number of shared classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = dataset[\"metadata\"][\"num_tasks\"]\n",
    "for i in range(num_tasks):\n",
    "    for j in range(i + 1, num_tasks):\n",
    "        task_i_classes = set(dataset[f\"task_{i}_train\"][\"fine_label\"])\n",
    "        task_j_classes = set(dataset[f\"task_{j}_train\"][\"fine_label\"])\n",
    "\n",
    "        num_shared_classes = len(task_i_classes.intersection(task_j_classes))\n",
    "        assert num_shared_classes == dataset[\"metadata\"][\"num_shared_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"metadata\"])\n",
    "num_shared_samples = dataset[\"metadata\"][\"num_train_samples_per_class\"] * dataset[\"metadata\"][\"num_shared_classes\"]\n",
    "print(num_shared_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = f\"task_{0}_train\"\n",
    "\n",
    "# shared samples are the same across all tasks\n",
    "shared_sample_embeddings = dataset[key][\"rexnet_100\"][0:num_shared_samples]\n",
    "all_sample_embeddings = [shared_sample_embeddings]\n",
    "\n",
    "for i in tqdm(range(num_tasks)):\n",
    "    key = f\"task_{i}_train\"\n",
    "\n",
    "    # (num_task_samples, embedding_dim)\n",
    "    task_i_novel_embeddings = dataset[key][\"rexnet_100\"][num_shared_samples:]\n",
    "\n",
    "    all_sample_embeddings.append(task_i_novel_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_samples, embedding_dim)\n",
    "original_space = torch.cat([torch.Tensor(sample_embedding) for sample_embedding in all_sample_embeddings], dim=0)\n",
    "print(original_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get shared samples indices\n",
    "Get the indices of samples from the shared classes, we will sample anchors only from these ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_classes = set(dataset[\"metadata\"][\"shared_classes\"])\n",
    "\n",
    "samples = dataset[\"task_0_train\"]\n",
    "labels = dataset[\"task_0_train\"][\"fine_label\"]\n",
    "\n",
    "shared_indices = []\n",
    "\n",
    "for ind, sample in tqdm(enumerate(samples)):\n",
    "    if labels[ind] in shared_classes:\n",
    "        shared_indices.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = 40000\n",
    "assert shared_indices == list(range(0, num_shared_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get non shared samples indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shared_indices = set(range(len(samples))).difference(shared_indices)\n",
    "print(len(non_shared_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = 40000\n",
    "num_novel_samples = 2500\n",
    "num_samples_per_task = num_shared_samples + num_novel_samples\n",
    "assert list(non_shared_indices) == list(range(num_shared_samples, num_samples_per_task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample anchor indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anchors = 512\n",
    "shared_anchor_indices = random.sample(shared_indices, num_anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = []\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(num_tasks)):\n",
    "    key = f\"task_{i}_train\"\n",
    "\n",
    "    # (num_task_samples, embedding_dim)\n",
    "    task_i_embeddings = torch.Tensor(dataset[key][\"rexnet_100\"])\n",
    "\n",
    "    # (num_anchors, embedding_dim)\n",
    "    task_i_anchors = task_i_embeddings[shared_anchor_indices]\n",
    "\n",
    "    embeddings.append(task_i_embeddings)\n",
    "    anchors.append(task_i_anchors)\n",
    "\n",
    "print(anchors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the anchors are the same across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tasks):\n",
    "    for j in range(i, num_tasks):\n",
    "        assert torch.all(torch.eq(anchors[i], anchors[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map to relative spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatives = []\n",
    "\n",
    "for i in range(num_tasks):\n",
    "    key = f\"task_{i}_train\"\n",
    "\n",
    "    abs_space = F.normalize(embeddings[i], p=2, dim=-1)\n",
    "    norm_anchors = F.normalize(anchors[i], p=2, dim=-1)\n",
    "\n",
    "    rel_space = abs_space @ norm_anchors.T\n",
    "    relatives.append(rel_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide shared samples and novel samples for each space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = 40000\n",
    "\n",
    "shared_samples = []\n",
    "disjoint_samples = []\n",
    "\n",
    "for relative in relatives:\n",
    "\n",
    "    task_i_shared = relative[0:num_shared_samples]\n",
    "    task_i_disjoint = relative[num_shared_samples:]\n",
    "\n",
    "    shared_samples.append(task_i_shared)\n",
    "    disjoint_samples.append(task_i_disjoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the shared samples are the same across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tasks):\n",
    "    for j in range(i, num_tasks):\n",
    "        assert torch.all(torch.eq(shared_samples[i], shared_samples[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the disjoint samples and the shared samples to go to the merged space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disjoint_samples = torch.cat(disjoint_samples, dim=0)\n",
    "all_disjoint_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space = torch.cat((shared_samples[0], all_disjoint_samples), dim=0)\n",
    "merged_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project original space to relative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_original_space = F.normalize(original_space, p=2, dim=-1)\n",
    "\n",
    "original_rel_space = abs_original_space @ norm_anchors.T\n",
    "print(original_rel_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(original_rel_space, merged_space, rtol=1e-05, atol=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
