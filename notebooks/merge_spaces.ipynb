{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from nn_core.common import PROJECT_ROOT\n",
    "from os import getcwd\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# from la.utils.utils import MyDatasetDict\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "try:\n",
    "    # be ready for 3.10 when it drops\n",
    "    from enum import StrEnum\n",
    "except ImportError:\n",
    "    from backports.strenum import StrEnum\n",
    "# from pytorch_lightning import seed_everything\n",
    "# import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "# import timm\n",
    "# from transformers import AutoModel, AutoProcessor\n",
    "# from typing import Sequence, List\n",
    "# from PIL.Image import Image\n",
    "from tqdm import tqdm\n",
    "# import functools\n",
    "# from timm.data import resolve_data_config\n",
    "# from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "# from timm.data import create_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amahmed/Desktop/UMass/Thesis/models/latent-aggregation\n",
      "/Users/amahmed/Desktop/UMass/Thesis/models/latent-aggregation/notebooks\n"
     ]
    }
   ],
   "source": [
    "# PROJECT_ROOT = '/'.join(getcwd().split('/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'fine_label', 'coarse_label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'fine_label', 'coarse_label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET_DIR: Path = PROJECT_ROOT / \"data\" / \"cifar100_tasks\"\n",
    "dataset = load_dataset(\"cifar100\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that all the tasks only have the desired number of shared classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_tasks \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tasks):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_tasks):\n",
      "File \u001b[0;32m~/Desktop/UMass/Thesis/models/.venv/lib/python3.12/site-packages/datasets/dataset_dict.py:75\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     78\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     79\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'metadata'"
     ]
    }
   ],
   "source": [
    "num_tasks = dataset[\"metadata\"][\"num_tasks\"]\n",
    "for i in range(num_tasks):\n",
    "    for j in range(i + 1, num_tasks):\n",
    "        task_i_classes = set(dataset[f\"task_{i}_train\"][\"fine_label\"])\n",
    "        task_j_classes = set(dataset[f\"task_{j}_train\"][\"fine_label\"])\n",
    "\n",
    "        num_shared_classes = len(task_i_classes.intersection(task_j_classes))\n",
    "        assert num_shared_classes == dataset[\"metadata\"][\"num_shared_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"metadata\"])\n",
    "num_shared_samples = dataset[\"metadata\"][\"num_train_samples_per_class\"] * dataset[\"metadata\"][\"num_shared_classes\"]\n",
    "print(num_shared_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = f\"task_{0}_train\"\n",
    "\n",
    "# shared samples are the same across all tasks\n",
    "shared_sample_embeddings = dataset[key][\"rexnet_100\"][0:num_shared_samples]\n",
    "all_sample_embeddings = [shared_sample_embeddings]\n",
    "\n",
    "for i in tqdm(range(num_tasks)):\n",
    "    key = f\"task_{i}_train\"\n",
    "\n",
    "    # (num_task_samples, embedding_dim)\n",
    "    task_i_novel_embeddings = dataset[key][\"rexnet_100\"][num_shared_samples:]\n",
    "\n",
    "    all_sample_embeddings.append(task_i_novel_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_samples, embedding_dim)\n",
    "original_space = torch.cat([torch.Tensor(sample_embedding) for sample_embedding in all_sample_embeddings], dim=0)\n",
    "print(original_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get shared samples indices\n",
    "Get the indices of samples from the shared classes, we will sample anchors only from these ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_classes = set(dataset[\"metadata\"][\"shared_classes\"])\n",
    "\n",
    "samples = dataset[\"task_0_train\"]\n",
    "labels = dataset[\"task_0_train\"][\"fine_label\"]\n",
    "\n",
    "shared_indices = []\n",
    "\n",
    "for ind, sample in tqdm(enumerate(samples)):\n",
    "    if labels[ind] in shared_classes:\n",
    "        shared_indices.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = 40000\n",
    "assert shared_indices == list(range(0, num_shared_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get non shared samples indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shared_indices = set(range(len(samples))).difference(shared_indices)\n",
    "print(len(non_shared_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = 40000\n",
    "num_novel_samples = 2500\n",
    "num_samples_per_task = num_shared_samples + num_novel_samples\n",
    "assert list(non_shared_indices) == list(range(num_shared_samples, num_samples_per_task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample anchor indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anchors = 512\n",
    "shared_anchor_indices = random.sample(shared_indices, num_anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = []\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(num_tasks)):\n",
    "    key = f\"task_{i}_train\"\n",
    "\n",
    "    # (num_task_samples, embedding_dim)\n",
    "    task_i_embeddings = torch.Tensor(dataset[key][\"rexnet_100\"])\n",
    "\n",
    "    # (num_anchors, embedding_dim)\n",
    "    task_i_anchors = task_i_embeddings[shared_anchor_indices]\n",
    "\n",
    "    embeddings.append(task_i_embeddings)\n",
    "    anchors.append(task_i_anchors)\n",
    "\n",
    "print(anchors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the anchors are the same across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tasks):\n",
    "    for j in range(i, num_tasks):\n",
    "        assert torch.all(torch.eq(anchors[i], anchors[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map to relative spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatives = []\n",
    "\n",
    "for i in range(num_tasks):\n",
    "    key = f\"task_{i}_train\"\n",
    "\n",
    "    abs_space = F.normalize(embeddings[i], p=2, dim=-1)\n",
    "    norm_anchors = F.normalize(anchors[i], p=2, dim=-1)\n",
    "\n",
    "    rel_space = abs_space @ norm_anchors.T\n",
    "    relatives.append(rel_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide shared samples and novel samples for each space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = 40000\n",
    "\n",
    "shared_samples = []\n",
    "disjoint_samples = []\n",
    "\n",
    "for relative in relatives:\n",
    "\n",
    "    task_i_shared = relative[0:num_shared_samples]\n",
    "    task_i_disjoint = relative[num_shared_samples:]\n",
    "\n",
    "    shared_samples.append(task_i_shared)\n",
    "    disjoint_samples.append(task_i_disjoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the shared samples are the same across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tasks):\n",
    "    for j in range(i, num_tasks):\n",
    "        assert torch.all(torch.eq(shared_samples[i], shared_samples[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the disjoint samples and the shared samples to go to the merged space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disjoint_samples = torch.cat(disjoint_samples, dim=0)\n",
    "all_disjoint_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space = torch.cat((shared_samples[0], all_disjoint_samples), dim=0)\n",
    "merged_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project original space to relative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_original_space = F.normalize(original_space, p=2, dim=-1)\n",
    "\n",
    "original_rel_space = abs_original_space @ norm_anchors.T\n",
    "print(original_rel_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(original_rel_space, merged_space, rtol=1e-05, atol=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
