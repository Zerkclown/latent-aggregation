{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from la.utils.utils import MyDatasetDict, add_tensor_column\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from la.pl_modules.pl_module import MyLightningModule\n",
    "from la.utils.utils import MyDatasetDict\n",
    "from backports.strenum import StrEnum\n",
    "from enum import auto\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "import hdf5storage\n",
    "from torch.nn.functional import mse_loss, pairwise_distance\n",
    "from torchmetrics.functional import pearson_corrcoef, spearman_corrcoef\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from datasets import concatenate_datasets, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tueplots import bundles\n",
    "\n",
    "seed_everything(43)\n",
    "bundles.icml2022()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"from_scratch_cnn\"\n",
    "dataset_name = \"cifar100\"\n",
    "\n",
    "dataset_path = f\"{PROJECT_ROOT}/data/{dataset_name}/partitioned_{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: MyDatasetDict = MyDatasetDict.load_from_disk(dataset_dict_path=dataset_path)\n",
    "num_tasks = data[\"metadata\"][\"num_tasks\"]\n",
    "num_anchors = len(data[\"task_0_anchors\"])\n",
    "num_classes = data[\"metadata\"][\"num_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPT) select a subset of the anchors\n",
    "SUBSAMPLE_ANCHORS = True\n",
    "\n",
    "if SUBSAMPLE_ANCHORS:\n",
    "    num_anchors = 256\n",
    "    for task in range(num_tasks + 1):\n",
    "        anchors_subsample = data[f\"task_{task}_anchors\"].select(range(num_anchors))\n",
    "        print(anchors_subsample)\n",
    "        data[f\"task_{task}_anchors\"] = anchors_subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in range(num_tasks + 1):\n",
    "    for mode in [\"train\", \"test\", \"anchors\"]:\n",
    "        data[f\"task_{task}_{mode}\"].set_format(\"torch\", columns=[\"embedding\", \"y\", \"id\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map to relative space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "\n",
    "    task_anchors = data[f\"task_{task_ind}_anchors\"][\"embedding\"]\n",
    "    norm_anchors = F.normalize(task_anchors, p=2, dim=-1)\n",
    "\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "\n",
    "        task_embeddings = data[f\"task_{task_ind}_{mode}\"][\"embedding\"]\n",
    "\n",
    "        abs_space = F.normalize(task_embeddings, p=2, dim=-1)\n",
    "\n",
    "        rel_space = abs_space @ norm_anchors.T\n",
    "\n",
    "        data[f\"task_{task_ind}_{mode}\"] = add_tensor_column(\n",
    "            data[f\"task_{task_ind}_{mode}\"], \"relative_embeddings\", rel_space\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "        data[f\"task_{task_ind}_{mode}\"].set_format(\n",
    "            type=\"torch\", columns=[\"relative_embeddings\", \"embedding\", \"y\", \"id\"]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = concatenate_datasets([data[f\"task_{i}_{mode}\"] for i in range(1, num_tasks + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_dataset[\"relative_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = data[f\"task_0_{mode}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = merged_dataset.sort(\"id\")\n",
    "original_dataset = data[f\"task_0_{mode}\"].sort(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space = merged_dataset[\"relative_embeddings\"]\n",
    "original_space = original_dataset[\"relative_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.cka import CKA\n",
    "\n",
    "cka = CKA(mode=\"linear\", device=\"cuda\")\n",
    "\n",
    "cka_score = cka(merged_space, original_space)\n",
    "print(cka_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(pytorch_lightning.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier: nn.Module,\n",
    "        use_relatives: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "        self.use_relatives = use_relatives\n",
    "        self.embedding_key = \"relative_embeddings\" if self.use_relatives else \"embedding\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[self.embedding_key], batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[self.embedding_key], batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_step=True, prog_bar=True)\n",
    "\n",
    "        val_acc = self.accuracy(y_hat, y)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch[self.embedding_key], batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True)\n",
    "\n",
    "        test_acc = self.accuracy(y_hat, y)\n",
    "        self.log(\"test_acc\", test_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from la.utils.class_analysis import Classifier\n",
    "\n",
    "\n",
    "def run_classification_experiment(\n",
    "    num_total_classes,\n",
    "    input_dim,\n",
    "    dataset,\n",
    "    use_relatives,\n",
    "    classifier_embed_dim,\n",
    "):\n",
    "    seed_everything(42)\n",
    "\n",
    "    dataloader_func = partial(\n",
    "        torch.utils.data.DataLoader,\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    trainer_func = partial(Trainer, gpus=1, max_epochs=100, logger=False, enable_progress_bar=True)\n",
    "\n",
    "    classifier = Classifier(\n",
    "        input_dim=input_dim,\n",
    "        classifier_embed_dim=classifier_embed_dim,\n",
    "        num_classes=num_total_classes,\n",
    "    )\n",
    "    model = Model(\n",
    "        classifier=classifier,\n",
    "        use_relatives=use_relatives,\n",
    "    )\n",
    "    trainer = trainer_func(callbacks=[pytorch_lightning.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)])\n",
    "\n",
    "    # split dataset in train, val and test\n",
    "    split_dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    val_test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "    split_val_test = val_test_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "    val_dataset = split_val_test[\"train\"]\n",
    "    test_dataset = split_val_test[\"test\"]\n",
    "\n",
    "    train_dataloader = dataloader_func(train_dataset, shuffle=True)\n",
    "    val_dataloader = dataloader_func(val_dataset, shuffle=False)\n",
    "    test_dataloader = dataloader_func(test_dataset, shuffle=False)\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    results = trainer.test(model, test_dataloader)[0]\n",
    "\n",
    "    results = {\n",
    "        \"total_acc\": results[\"test_acc_epoch\"],\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_embed_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classification_experiment(\n",
    "    dataset=original_dataset,\n",
    "    use_relatives=False,\n",
    "    input_dim=original_dataset[\"embedding\"].shape[1],\n",
    "    num_total_classes=num_classes,\n",
    "    classifier_embed_dim=classifier_embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classification_experiment(\n",
    "    dataset=original_dataset,\n",
    "    use_relatives=True,\n",
    "    input_dim=num_anchors,\n",
    "    num_total_classes=num_classes,\n",
    "    classifier_embed_dim=classifier_embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classification_experiment(\n",
    "    dataset=merged_dataset,\n",
    "    use_relatives=True,\n",
    "    input_dim=num_anchors,\n",
    "    num_total_classes=num_classes,\n",
    "    classifier_embed_dim=classifier_embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
