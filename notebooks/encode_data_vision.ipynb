{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding CIFAR100 with a set of pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [HuggingFace datasets library](https://huggingface.co/datasets) to load the CIFAR100 dataset. We will then encode the images using a set of pretrained models from the [timm library](https://rwightman.github.io/pytorch-image-models/) and from Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # be ready for 3.10 when it drops\n",
    "    from enum import StrEnum\n",
    "except ImportError:\n",
    "    from backports.strenum import StrEnum\n",
    "from pytorch_lightning import seed_everything\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import timm\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from typing import Sequence, List\n",
    "from PIL.Image import Image\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "from timm.data import resolve_data_config\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "from timm.data import create_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHED: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name: str, split: str, perc: float, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Load a dataset from the HuggingFace datasets library.\n",
    "    \"\"\"\n",
    "    assert 0 < perc <= 1\n",
    "    dataset = load_dataset(\n",
    "        name,\n",
    "        split=split,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # Select a random subset\n",
    "    if perc != 1:\n",
    "        dataset = dataset.shuffle(seed=seed).select(list(range(int(len(dataset) * perc))))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetParams = namedtuple(\"DatasetParams\", [\"name\", \"fine_grained\", \"train_split\", \"test_split\", \"perc\", \"hf_key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params: DatasetParams = DatasetParams(\"cifar100\", None, \"train\", \"test\", 1, (\"cifar100\",))\n",
    "dataset_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_KEY = \"_\".join(map(str, [v for k, v in dataset_params._asdict().items() if k != \"hf_key\" and v is not None]))\n",
    "DATASET_DIR: Path = PROJECT_ROOT / \"data\" / \"encoded_data\" / DATASET_KEY\n",
    "DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATASET_DIR.exists() or not USE_CACHED:\n",
    "\n",
    "    data: DatasetDict = DatasetDict(\n",
    "        train=get_dataset(name=dataset_params.name, split=dataset_params.train_split, perc=dataset_params.perc),\n",
    "        test=get_dataset(name=dataset_params.name, split=dataset_params.test_split, perc=dataset_params.perc),\n",
    "    )\n",
    "else:\n",
    "    data: Dataset = load_from_disk(dataset_path=str(DATASET_DIR))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RECOMPUTE: bool = False\n",
    "DEVICE: str = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODERS = (\n",
    "    \"rexnet_100\",\n",
    "    \"vit_base_patch16_224\",\n",
    "    \"vit_base_patch16_384\",\n",
    "    \"vit_base_resnet50_384\",\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    \"vit_small_patch16_224\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_field(batch, src_field: str, tgt_field: str, transformation):\n",
    "    \"\"\"\n",
    "    Create a new field with name `tgt_field` by applying `transformation` to `src_field`.\n",
    "    \"\"\"\n",
    "    src_data = batch[src_field]\n",
    "    transformed = transformation(src_data)\n",
    "\n",
    "    return {tgt_field: transformed}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def image_encode(images: Sequence[Image], transform, encoder):\n",
    "    \"\"\"\n",
    "    Encode images using a timm model.\n",
    "    \"\"\"\n",
    "    images: List[torch.Tensor] = [transform(image.convert(\"RGB\")) for image in images]\n",
    "    images: torch.Tensor = torch.stack(images, dim=0).to(DEVICE)\n",
    "    encoding = encoder(images)\n",
    "\n",
    "    return list(encoding.cpu().numpy())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_image_encode(images: Sequence[Image], transform, encoder):\n",
    "    \"\"\"\n",
    "    Encode images using the OpenAI CLIP model.\n",
    "    \"\"\"\n",
    "    images = [image.convert(\"RGB\") for image in images]\n",
    "    image_inputs = transform(images=images, return_tensors=\"pt\").to(DEVICE)\n",
    "    encoder_out = encoder.vision_model(**image_inputs)\n",
    "    encoding = encoder_out.pooler_output\n",
    "\n",
    "    return list(encoding.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_encoders = [encoder for encoder in ENCODERS if FORCE_RECOMPUTE or encoder not in data[\"train\"].column_names]\n",
    "\n",
    "for encoder_name in tqdm(missing_encoders):\n",
    "    tgt_field: str = encoder_name\n",
    "\n",
    "    if encoder_name.startswith(\"openai/clip\"):\n",
    "        encoder = AutoModel.from_pretrained(encoder_name).requires_grad_(False).eval().to(DEVICE)\n",
    "        transform = AutoProcessor.from_pretrained(encoder_name)\n",
    "        encode_func = clip_image_encode\n",
    "\n",
    "    else:\n",
    "        encoder = (\n",
    "            timm.create_model(encoder_name, pretrained=True, num_classes=0).requires_grad_(False).eval().to(DEVICE)\n",
    "        )\n",
    "        config = resolve_data_config({}, model=encoder)\n",
    "        transform = create_transform(**config)\n",
    "        encode_func = image_encode\n",
    "\n",
    "    data = data.map(\n",
    "        functools.partial(\n",
    "            encode_field,\n",
    "            src_field=\"img\",\n",
    "            tgt_field=tgt_field,\n",
    "            transformation=functools.partial(\n",
    "                encode_func,\n",
    "                transform=transform,\n",
    "                encoder=encoder,\n",
    "            ),\n",
    "        ),\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        desc=f\"{encoder_name}\",\n",
    "    )\n",
    "    encoder = encoder.cpu()\n",
    "\n",
    "    data.save_to_disk(str(DATASET_DIR))\n",
    "\n",
    "if \"index\" not in data[\"train\"].column_names:\n",
    "    data = data.map(lambda x, index: {\"index\": index}, with_indices=True)\n",
    "    data.save_to_disk(str(DATASET_DIR))\n",
    "\n",
    "data.set_format(columns=ENCODERS, output_all_columns=True, type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rel2abs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b686ed3b4db589ef348e6d48f4693659d5f908f7b72a8be682d26bd45ec8cc07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
