{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # be ready for 3.10 when it drops\n",
    "    from enum import StrEnum\n",
    "except ImportError:\n",
    "    from backports.strenum import StrEnum\n",
    "from pytorch_lightning import seed_everything\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import timm\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from typing import Sequence, List\n",
    "from PIL.Image import Image\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "from timm.data import resolve_data_config\n",
    "from datasets import load_dataset, load_from_disk, Dataset, DatasetDict\n",
    "import torchvision\n",
    "import torch\n",
    "from timm.data import create_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: MyDatasetDict = MyDatasetDict.load_from_disk(\"../data/cifar100/partitioned\")\n",
    "\n",
    "num_tasks = data[\"metadata\"][\"num_tasks\"]\n",
    "\n",
    "for task_ind in range(num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_anchors\"] = data[\"anchors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_func = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in {\"train\", \"test\", \"anchors\"}:\n",
    "    for task_ind in range(num_tasks + 1):\n",
    "        data[f\"task_{task_ind}_{mode}\"] = data[f\"task_{task_ind}_{mode}\"].map(\n",
    "            lambda x: {\"x\": transform_func(x[\"x\"])}, batched=False\n",
    "        )\n",
    "        data[f\"task_{task_ind}_{mode}\"].set_format(\"torch\", columns=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Sequence, Tuple, Union, Dict\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from nn_core.model_logging import NNLogger\n",
    "from omegaconf import DictConfig\n",
    "from torch.optim import Optimizer\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from la.pl_modules.pl_module import MyLightningModule\n",
    "\n",
    "pylogger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ResNet(MyLightningModule):\n",
    "    logger: NNLogger\n",
    "\n",
    "    def __init__(self, num_classes, input_dim, model, *args, **kwargs) -> None:\n",
    "        super().__init__(num_classes=num_classes, input_dim=input_dim, *args, **kwargs)\n",
    "\n",
    "        self.save_hyperparameters(logger=False, ignore=(\"metadata\",))\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Dict:\n",
    "        \"\"\"Method for the forward pass.\n",
    "\n",
    "        'training_step', 'validation_step' and 'test_step' should call\n",
    "        this method in order to compute the output predictions and the loss.\n",
    "\n",
    "        Returns:\n",
    "            output_dict: forward output containing the predictions (output logits ecc...) and the loss if any.\n",
    "        \"\"\"\n",
    "        model_out = self.model(x)\n",
    "\n",
    "        return model_out\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Union[Optimizer, Tuple[Sequence[Optimizer], Sequence[Any]]]:\n",
    "        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "\n",
    "        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n",
    "\n",
    "        Return:\n",
    "            Any of these 6 options.\n",
    "            - Single optimizer.\n",
    "            - List or Tuple - List of optimizers.\n",
    "            - Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).\n",
    "            - Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler'\n",
    "              key whose value is a single LR scheduler or lr_dict.\n",
    "            - Tuple of dictionaries as described, with an optional 'frequency' key.\n",
    "            - None - Fit will run without any optimizer.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = [\"ResNet\", \"resnet20\", \"resnet32\", \"resnet44\", \"resnet56\", \"resnet110\", \"resnet1202\"]\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == \"A\":\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes // 4, planes // 4), \"constant\", 0)\n",
    "                )\n",
    "            elif option == \"B\":\n",
    "                self.shortcut = nn.Sequential(\n",
    "                    nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm2d(self.expansion * planes),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetModule(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNetModule, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        embeds = out.view(out.size(0), -1)\n",
    "        out = F.relu(embeds)\n",
    "\n",
    "        out = self.linear(embeds)\n",
    "        return {\"embeds\": embeds, \"logits\": out}\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, test_data):\n",
    "    model = ResNet(\n",
    "        num_classes=100, model=ResNetModule(BasicBlock, [3, 3, 3], num_classes=100), input_dim=32, transform_func=None\n",
    "    )\n",
    "\n",
    "    model.configure_optimizers()\n",
    "\n",
    "    dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=1,\n",
    "        max_epochs=1,\n",
    "        precision=32,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    test_dataloader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    trainer.test(model, test_dataloader)\n",
    "\n",
    "    model = model.cuda()\n",
    "    test_embeds = []\n",
    "    for batch in test_dataloader:\n",
    "        x, y = batch[\"x\"], batch[\"y\"]\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        embeds = model(x)[\"embeds\"]\n",
    "        test_embeds.append(embeds.detach().cpu().numpy())\n",
    "\n",
    "    return model, test_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, test_embeds1 = train_model(data[\"task_1_train\"], data[\"task_1_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = train_model(data[\"task_2_train\"], data[\"task_2_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
