{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from la.modules.module import CNN\n",
    "from la.pl_modules.pl_module import MyLightningModule\n",
    "from la.utils.utils import MyDatasetDict\n",
    "from backports.strenum import StrEnum\n",
    "from enum import auto\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "import hdf5storage\n",
    "from torch.nn.functional import mse_loss, pairwise_distance\n",
    "from torchmetrics.functional import pearson_corrcoef, spearman_corrcoef\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tueplots import bundles\n",
    "\n",
    "seed_everything(43)\n",
    "bundles.icml2022()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"../conf\")\n",
    "cfg = compose(config_name=\"aggr_exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: MyDatasetDict = MyDatasetDict.load_from_disk(dataset_dict_path=str(cfg.nn.output_path))\n",
    "num_tasks = data[\"metadata\"][\"num_tasks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on training samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the two datasets in the same way using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].sort(\"id\")\n",
    "    data[f\"task_{task_ind}_test\"] = data[f\"task_{task_ind}_test\"].sort(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspaces, embeddings from classifiers trained on a subset of the classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the local labels back to global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(1, num_tasks + 1):\n",
    "\n",
    "    global_to_local_map = data[\"metadata\"][\"global_to_local_class_mappings\"][f\"task_{task_ind}\"]\n",
    "    local_to_global_map = {v: int(k) for k, v in global_to_local_map.items()}\n",
    "\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "        lambda row: {\"fine_label\": local_to_global_map[row[\"fine_label\"].item()]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get shared samples indices\n",
    "Get the indices of samples from the shared classes, we will sample anchors only from these ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = data[\"metadata\"][\"num_train_samples_per_class\"] * data[\"metadata\"][\"num_shared_classes\"]\n",
    "shared_classes = set(data[\"metadata\"][\"shared_classes\"])\n",
    "\n",
    "for task_ind in range(num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "        lambda row: {\"shared\": row[\"fine_label\"].item() in shared_classes}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get non shared samples indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_per_task = data[\"metadata\"][\"num_shared_classes\"] + data[\"metadata\"][\"num_novel_classes_per_task\"]\n",
    "num_train_samples_per_task = data[\"metadata\"][\"num_train_samples_per_class\"] * num_classes_per_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample anchor indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anchors = 256\n",
    "\n",
    "shared_indices = []\n",
    "\n",
    "for task_ind in range(num_tasks + 1):\n",
    "    # get the indices of samples having shared to True\n",
    "    task_shared_indices = data[f\"task_{task_ind}_train\"][\"id\"][data[f\"task_{task_ind}_train\"][\"shared\"]].tolist()\n",
    "    shared_indices.append(task_shared_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks + 1):\n",
    "    for task_j in range(task_i, num_tasks + 1):\n",
    "        assert shared_indices[task_i] == shared_indices[task_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_indices = random.sample(shared_indices[0], num_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "        lambda row: {\"anchor\": row[\"id\"].item() in anchor_indices}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centering = True\n",
    "if centering:\n",
    "    for task_ind in range(num_tasks + 1):\n",
    "        embedding_mean = data[f\"task_{task_ind}_train\"][\"embedding\"].mean(dim=0)\n",
    "        data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "            lambda row: {\"embedding\": row[\"embedding\"] - embedding_mean}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = []\n",
    "for i in tqdm(range(0, num_tasks + 1)):\n",
    "    task_i_anchors = data[f\"task_{i}_train\"][\"embedding\"][data[f\"task_{i}_train\"][\"anchor\"]]\n",
    "    anchors.append(task_i_anchors)\n",
    "\n",
    "print(anchors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map to relative spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatives = []\n",
    "\n",
    "for task_ind in range(0, num_tasks + 1):\n",
    "    task_embeddings = data[f\"task_{task_ind}_train\"][\"embedding\"]\n",
    "    task_anchors = anchors[task_ind]\n",
    "\n",
    "    abs_space = F.normalize(task_embeddings, p=2, dim=-1)\n",
    "    norm_anchors = F.normalize(task_anchors, p=2, dim=-1)\n",
    "\n",
    "    rel_space = abs_space @ norm_anchors.T\n",
    "\n",
    "    # _, _, Vt = torch.linalg.svd(norm_anchors)\n",
    "\n",
    "    # Project all X onto the anchor-space\n",
    "    # rel_space = torch.einsum(\"nd,ad -> na\", abs_space, Vt)\n",
    "\n",
    "    relatives.append(rel_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "for task_ind in range(0, num_tasks + 1):\n",
    "\n",
    "    dataset_dict = data[f\"task_{task_ind}_train\"].to_dict()\n",
    "\n",
    "    dataset_dict[\"relative_embeddings\"] = relatives[task_ind].tolist()\n",
    "\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "    data[f\"task_{task_ind}_train\"] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del relatives\n",
    "del dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"].set_format(\n",
    "        type=\"torch\", columns=[\"relative_embeddings\", \"embedding\", \"fine_label\", \"id\", \"shared\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average the shared samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_samples = []\n",
    "disjoint_samples = []\n",
    "\n",
    "for task_ind in range(1, num_tasks + 1):\n",
    "    task_shared_samples = data[f\"task_{task_ind}_train\"].filter(lambda row: row[\"shared\"]).sort(\"id\")\n",
    "\n",
    "    task_novel_samples = data[f\"task_{task_ind}_train\"].filter(lambda row: ~row[\"shared\"])\n",
    "\n",
    "    shared_samples.append(task_shared_samples)\n",
    "    disjoint_samples.append(task_novel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "novel_samples = concatenate_datasets(disjoint_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del disjoint_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(1, num_tasks + 1):\n",
    "    del data[f\"task_{task_ind}_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of the shared_samples and put them back in the dataset\n",
    "# Extract the 'embedding' columns from each dataset\n",
    "\n",
    "shared_rel_embeddings = [dataset[\"relative_embeddings\"] for dataset in shared_samples]\n",
    "\n",
    "# Calculate the mean of the embeddings for each sample\n",
    "mean_embeddings = torch.mean(torch.stack(shared_rel_embeddings), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset with the same features as the original datasets\n",
    "new_features = shared_samples[0].features.copy()\n",
    "\n",
    "# Replace the 'embedding' column in the new dataset with the mean embeddings\n",
    "new_data = {column: shared_samples[0][column] for column in new_features}\n",
    "new_data[\"relative_embeddings\"] = mean_embeddings.tolist()\n",
    "\n",
    "# Create the new Hugging Face dataset\n",
    "shared_dataset = Dataset.from_dict(new_data, features=new_features)\n",
    "del new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"??\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the task-specific samples and the shared samples to go to the merged space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = concatenate_datasets([shared_dataset, novel_samples])\n",
    "del shared_dataset\n",
    "del novel_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = merged_dataset.sort(\"id\")\n",
    "original_dataset = data[f\"task_0_train\"].sort(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.set_format(type=\"torch\", columns=[\"relative_embeddings\", \"fine_label\", \"coarse_label\"])\n",
    "original_dataset.set_format(type=\"torch\", columns=[\"relative_embeddings\", \"fine_label\", \"coarse_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space = merged_dataset[\"relative_embeddings\"]\n",
    "original_space = original_dataset[\"relative_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_space_y = original_dataset[\"fine_label\"]\n",
    "original_space_coarse_labels = original_dataset[\"coarse_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_dim: int = 1000\n",
    "subsample_indices = random.sample(range(0, original_space.shape[0]), subsample_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_original = original_space[subsample_indices]\n",
    "subsample_merged = merged_space[subsample_indices]\n",
    "subsample_labels = original_space_y[subsample_indices]\n",
    "subsample_coarse_labels = original_space_coarse_labels[subsample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices: torch.Tensor = subsample_labels.sort().indices\n",
    "\n",
    "subsample_original_sorted: torch.Tensor = subsample_original[sort_indices]\n",
    "subsample_merged_sorted: torch.Tensor = subsample_merged[sort_indices]\n",
    "subsample_labels_sorted: torch.Tensor = subsample_labels[sort_indices]\n",
    "subsample_coarse_labels_sorted: torch.Tensor = subsample_coarse_labels[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_pairwise_dist\n",
    "\n",
    "plot_pairwise_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import self_sim_comparison\n",
    "\n",
    "self_sim_comparison(space1=subsample_original_sorted, space2=subsample_merged_sorted, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_self_dist\n",
    "\n",
    "plot_self_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import Reduction, reduce\n",
    "\n",
    "x_header = [reduction.upper() for reduction in Reduction]\n",
    "y_header = [\"Relative Space 1\", \"Relative Space 2\"]\n",
    "\n",
    "spaces = [\n",
    "    [\n",
    "        *reduce(space1=subsample_original_sorted, space2=subsample_merged_sorted, reduction=reduction),\n",
    "    ]\n",
    "    for reduction in Reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_space_grid\n",
    "\n",
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_space_grid\n",
    "\n",
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_coarse_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
