{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pytorch_lightning\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from la.modules.efficient_net import MyEfficientNet\n",
    "from la.pl_modules.pl_module import MyLightningModule\n",
    "from la.utils.utils import MyDatasetDict\n",
    "from backports.strenum import StrEnum\n",
    "from enum import auto\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "import hdf5storage\n",
    "from torch.nn.functional import mse_loss, pairwise_distance\n",
    "from torchmetrics.functional import pearson_corrcoef, spearman_corrcoef\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tueplots import bundles\n",
    "\n",
    "seed_everything(43)\n",
    "bundles.icml2022()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"tiny_imagenet\"\n",
    "num_shared_classes = 100\n",
    "num_novel_classes = 20\n",
    "\n",
    "dataset_path = f\"{PROJECT_ROOT}/data/{dataset_name}/S{num_shared_classes}_N{num_novel_classes}_embedded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: MyDatasetDict = MyDatasetDict.load_from_disk(dataset_dict_path=dataset_path)\n",
    "num_tasks = data[\"metadata\"][\"num_tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on training samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the two datasets in the same way using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].sort(\"id\")\n",
    "    data[f\"task_{task_ind}_test\"] = data[f\"task_{task_ind}_test\"].sort(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspaces, embeddings from classifiers trained on a subset of the classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the local labels back to global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(1, num_tasks + 1):\n",
    "\n",
    "    global_to_local_map = data[\"metadata\"][\"global_to_local_class_mappings\"][f\"task_{task_ind}\"]\n",
    "    local_to_global_map = {v: int(k) for k, v in global_to_local_map.items()}\n",
    "\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "        lambda row: {\"y\": local_to_global_map[row[\"y\"].item()]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get shared samples indices\n",
    "Get the indices of samples from the shared classes, we will sample anchors only from these ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shared_samples = data[\"metadata\"][\"num_train_samples_per_class\"] * data[\"metadata\"][\"num_shared_classes\"]\n",
    "shared_classes = set(data[\"metadata\"][\"shared_classes\"])\n",
    "\n",
    "for task_ind in range(num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "        lambda row: {\"shared\": row[\"y\"].item() in shared_classes}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get non shared samples indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_per_task = data[\"metadata\"][\"num_shared_classes\"] + data[\"metadata\"][\"num_novel_classes_per_task\"]\n",
    "num_train_samples_per_task = data[\"metadata\"][\"num_train_samples_per_class\"] * num_classes_per_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample anchor indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anchors = 256\n",
    "\n",
    "shared_ids = []\n",
    "\n",
    "for task_ind in range(num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"].set_format(type=\"torch\", columns=[\"id\"])\n",
    "\n",
    "    # get the indices of samples having shared to True\n",
    "    shared_map = data[f\"task_{task_ind}_train\"][\"shared\"]\n",
    "    ids = data[f\"task_{task_ind}_train\"][\"id\"]\n",
    "    task_shared_ids = ids[shared_map].tolist()\n",
    "    shared_ids.append(task_shared_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks + 1):\n",
    "    for task_j in range(task_i, num_tasks + 1):\n",
    "        assert shared_ids[task_i] == shared_ids[task_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_ids = random.sample(shared_ids[0], num_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "        lambda row: {\"anchor\": row[\"id\"].item() in anchor_ids}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centering = False\n",
    "if centering:\n",
    "    for task_ind in range(num_tasks + 1):\n",
    "        embedding_mean = data[f\"task_{task_ind}_train\"][\"embedding\"].mean(dim=0)\n",
    "        data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "            lambda row: {\"embedding\": row[\"embedding\"] - embedding_mean}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = []\n",
    "for i in tqdm(range(0, num_tasks + 1)):\n",
    "    data[f\"task_{i}_train\"].set_format(type=\"torch\", columns=[\"embedding\"])\n",
    "    task_i_anchors = data[f\"task_{i}_train\"][\"embedding\"][data[f\"task_{i}_train\"][\"anchor\"]]\n",
    "    anchors.append(task_i_anchors)\n",
    "\n",
    "print(anchors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map to relative spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatives = []\n",
    "\n",
    "for task_ind in range(0, num_tasks + 1):\n",
    "    task_embeddings = data[f\"task_{task_ind}_train\"][\"embedding\"]\n",
    "    task_anchors = anchors[task_ind]\n",
    "\n",
    "    abs_space = F.normalize(task_embeddings, p=2, dim=-1)\n",
    "    norm_anchors = F.normalize(task_anchors, p=2, dim=-1)\n",
    "\n",
    "    rel_space = abs_space @ norm_anchors.T\n",
    "\n",
    "    # _, _, Vt = torch.linalg.svd(norm_anchors)\n",
    "\n",
    "    # Project all X onto the anchor-space\n",
    "    # rel_space = torch.einsum(\"nd,ad -> na\", abs_space, Vt)\n",
    "\n",
    "    relatives.append(rel_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "for task_ind in range(0, num_tasks + 1):\n",
    "\n",
    "    dataset_dict = data[f\"task_{task_ind}_train\"].to_dict()\n",
    "\n",
    "    dataset_dict[\"relative_embeddings\"] = relatives[task_ind].tolist()\n",
    "\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "    data[f\"task_{task_ind}_train\"] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del relatives\n",
    "del dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"].set_format(\n",
    "        type=\"torch\", columns=[\"relative_embeddings\", \"embedding\", \"y\", \"id\", \"shared\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average the shared samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_samples = []\n",
    "disjoint_samples = []\n",
    "\n",
    "for task_ind in range(1, num_tasks + 1):\n",
    "    task_shared_samples = data[f\"task_{task_ind}_train\"].filter(lambda row: row[\"shared\"]).sort(\"id\")\n",
    "\n",
    "    task_novel_samples = data[f\"task_{task_ind}_train\"].filter(lambda row: ~row[\"shared\"])\n",
    "\n",
    "    shared_samples.append(task_shared_samples)\n",
    "    disjoint_samples.append(task_novel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_samples = concatenate_datasets(disjoint_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del disjoint_samples\n",
    "\n",
    "for task_ind in range(1, num_tasks + 1):\n",
    "    del data[f\"task_{task_ind}_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of the shared_samples and put them back in the dataset\n",
    "# Extract the 'embedding' columns from each dataset\n",
    "\n",
    "shared_rel_embeddings = [dataset[\"relative_embeddings\"] for dataset in shared_samples]\n",
    "\n",
    "# Calculate the mean of the embeddings for each sample\n",
    "mean_embeddings = torch.mean(torch.stack(shared_rel_embeddings), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset with the same features as the original datasets\n",
    "new_features = shared_samples[0].features.copy()\n",
    "\n",
    "# Replace the 'embedding' column in the new dataset with the mean embeddings\n",
    "new_data = {column: shared_samples[0][column] for column in new_features}\n",
    "new_data[\"relative_embeddings\"] = mean_embeddings.tolist()\n",
    "\n",
    "# Create the new Hugging Face dataset\n",
    "shared_dataset = Dataset.from_dict(new_data, features=new_features)\n",
    "del new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the task-specific samples and the shared samples to go to the merged space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = concatenate_datasets([shared_dataset, novel_samples])\n",
    "del shared_dataset\n",
    "del novel_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = merged_dataset.sort(\"id\")\n",
    "original_dataset = data[f\"task_0_train\"].sort(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.set_format(type=\"torch\", columns=[\"relative_embeddings\", \"fine_label\", \"coarse_label\"])\n",
    "original_dataset.set_format(type=\"torch\", columns=[\"relative_embeddings\", \"fine_label\", \"coarse_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space = merged_dataset[\"relative_embeddings\"]\n",
    "original_space = original_dataset[\"relative_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_space_y = original_dataset[\"fine_label\"]\n",
    "original_space_coarse_labels = original_dataset[\"coarse_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole space (all classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_dim: int = 1000\n",
    "subsample_indices = random.sample(range(0, original_space.shape[0]), subsample_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_original = original_space[subsample_indices]\n",
    "subsample_merged = merged_space[subsample_indices]\n",
    "subsample_labels = original_space_y[subsample_indices]\n",
    "subsample_coarse_labels = original_space_coarse_labels[subsample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices: torch.Tensor = subsample_labels.sort().indices\n",
    "\n",
    "subsample_original_sorted: torch.Tensor = subsample_original[sort_indices]\n",
    "subsample_merged_sorted: torch.Tensor = subsample_merged[sort_indices]\n",
    "subsample_labels_sorted: torch.Tensor = subsample_labels[sort_indices]\n",
    "subsample_coarse_labels_sorted: torch.Tensor = subsample_coarse_labels[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_pairwise_dist\n",
    "\n",
    "plot_pairwise_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import self_sim_comparison\n",
    "\n",
    "self_sim_comparison(space1=subsample_original_sorted, space2=subsample_merged_sorted, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_self_dist\n",
    "\n",
    "plot_self_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import Reduction, reduce\n",
    "\n",
    "x_header = [reduction.upper() for reduction in Reduction]\n",
    "y_header = [\"Relative Space 1\", \"Relative Space 2\"]\n",
    "\n",
    "spaces = [\n",
    "    [\n",
    "        *reduce(space1=subsample_original_sorted, space2=subsample_merged_sorted, reduction=reduction),\n",
    "    ]\n",
    "    for reduction in Reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_space_grid\n",
    "\n",
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_space_grid\n",
    "\n",
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_coarse_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only non-shared classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shared_classes = set(range(100)).difference(data[\"metadata\"][\"shared_classes\"])\n",
    "non_shared_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_nonshared = merged_dataset.filter(lambda row: row[\"fine_label\"].item() in non_shared_classes)\n",
    "original_dataset_nonshared = original_dataset.filter(lambda row: row[\"fine_label\"].item() in non_shared_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space_nonshared = merged_dataset_nonshared[\"relative_embeddings\"]\n",
    "original_space_nonshared = original_dataset_nonshared[\"relative_embeddings\"]\n",
    "original_space_y_nonshared = original_dataset_nonshared[\"fine_label\"]\n",
    "original_space_coarse_labels_nonshared = original_dataset_nonshared[\"coarse_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_dim: int = 1000\n",
    "subsample_indices = random.sample(range(0, original_space_nonshared.shape[0]), subsample_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_original = original_space_nonshared[subsample_indices]\n",
    "subsample_merged = merged_space_nonshared[subsample_indices]\n",
    "subsample_labels = original_space_y_nonshared[subsample_indices]\n",
    "subsample_coarse_labels = original_space_coarse_labels_nonshared[subsample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices: torch.Tensor = subsample_labels.sort().indices\n",
    "\n",
    "subsample_original_sorted: torch.Tensor = subsample_original[sort_indices]\n",
    "subsample_merged_sorted: torch.Tensor = subsample_merged[sort_indices]\n",
    "subsample_labels_sorted: torch.Tensor = subsample_labels[sort_indices]\n",
    "subsample_coarse_labels_sorted: torch.Tensor = subsample_coarse_labels[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_pairwise_dist\n",
    "\n",
    "plot_pairwise_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import self_sim_comparison\n",
    "\n",
    "self_sim_comparison(space1=subsample_original_sorted, space2=subsample_merged_sorted, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_self_dist\n",
    "\n",
    "plot_self_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import Reduction, reduce\n",
    "\n",
    "x_header = [reduction.upper() for reduction in Reduction]\n",
    "y_header = [\"Relative Space 1\", \"Relative Space 2\"]\n",
    "\n",
    "spaces = [\n",
    "    [\n",
    "        *reduce(space1=subsample_original_sorted, space2=subsample_merged_sorted, reduction=reduction),\n",
    "    ]\n",
    "    for reduction in Reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_space_grid\n",
    "\n",
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_space_grid\n",
    "\n",
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_coarse_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import RichProgressBar, TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "dataloader_func = partial(\n",
    "    torch.utils.data.DataLoader,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "trainer_func = partial(Trainer, gpus=1, max_epochs=100, logger=False, enable_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_embed_dim = 512\n",
    "num_classes = 100\n",
    "\n",
    "\n",
    "def get_classifier():\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape=original_space.shape[1]),\n",
    "        nn.Linear(num_anchors, classifier_embed_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(classifier_embed_dim, num_classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, classifier: nn.Module):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"relative_embeddings\"], batch[\"fine_label\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"relative_embeddings\"], batch[\"fine_label\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_step=True, prog_bar=True)\n",
    "\n",
    "        val_acc = self.accuracy(y_hat, y)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch[\"relative_embeddings\"], batch[\"fine_label\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True)\n",
    "\n",
    "        test_acc = self.accuracy(y_hat, y)\n",
    "        self.log(\"test_acc\", test_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier over the original space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_classifier = Model(classifier=get_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_func(callbacks=[pytorch_lightning.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in train, val and test\n",
    "split_dataset = original_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "original_dataset_train = split_dataset[\"train\"]\n",
    "original_dataset_val_test = split_dataset[\"test\"]\n",
    "\n",
    "split_val_test = original_dataset_val_test.train_test_split(test_size=0.5, seed=42)\n",
    "original_dataset_val = split_val_test[\"train\"]\n",
    "original_dataset_test = split_val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_dataloader = dataloader_func(original_dataset_train, shuffle=True)\n",
    "original_val_dataloader = dataloader_func(original_dataset_val, shuffle=False)\n",
    "original_test_dataloader = dataloader_func(original_dataset_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(original_classifier, original_train_dataloader, original_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(original_classifier, original_test_dataloader)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier over the merged space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in train, val and test\n",
    "split_dataset = merged_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "merged_dataset_train = split_dataset[\"train\"]\n",
    "merged_dataset_val_test = split_dataset[\"test\"]\n",
    "\n",
    "split_val_test = merged_dataset_val_test.train_test_split(test_size=0.5, seed=42)\n",
    "merged_dataset_val = split_val_test[\"train\"]\n",
    "merged_dataset_test = split_val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_dataloader = dataloader_func(merged_dataset_train, shuffle=True)\n",
    "merged_val_dataloader = dataloader_func(merged_dataset_val, shuffle=False)\n",
    "merged_test_dataloader = dataloader_func(merged_dataset_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier()\n",
    "\n",
    "merged_classifier = Model(classifier=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_func(callbacks=[pytorch_lightning.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)])\n",
    "trainer.fit(merged_classifier, merged_train_dataloader, merged_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(merged_classifier, merged_test_dataloader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
