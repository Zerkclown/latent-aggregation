{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from la.utils.utils import MyDatasetDict, add_tensor_column\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from la.pl_modules.pl_module import MyLightningModule\n",
    "from la.utils.utils import MyDatasetDict\n",
    "from backports.strenum import StrEnum\n",
    "from enum import auto\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "import hdf5storage\n",
    "from torch.nn.functional import mse_loss, pairwise_distance\n",
    "from torchmetrics.functional import pearson_corrcoef, spearman_corrcoef\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from datasets import concatenate_datasets, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tueplots import bundles\n",
    "\n",
    "seed_everything(43)\n",
    "bundles.icml2022()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cifar100\"\n",
    "model_name = \"efficient_net\"\n",
    "num_shared_classes = 80\n",
    "num_novel_classes = 5\n",
    "num_total_classes = 100\n",
    "\n",
    "dataset_path = f\"{PROJECT_ROOT}/data/{dataset_name}/S{num_shared_classes}_N{num_novel_classes}_{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: MyDatasetDict = MyDatasetDict.load_from_disk(dataset_dict_path=dataset_path)\n",
    "num_tasks = data[\"metadata\"][\"num_tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shared_classes = set(range(num_total_classes)).difference(data[\"metadata\"][\"shared_classes\"])\n",
    "non_shared_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in range(num_tasks + 1):\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "        data[f\"task_{task}_{mode}\"].set_format(\"torch\", columns=[\"embedding\", \"y\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = data[\"metadata\"][\"num_tasks\"]\n",
    "\n",
    "shared_classes = set(data[\"metadata\"][\"shared_classes\"])\n",
    "\n",
    "num_shared_samples = data[\"metadata\"][\"num_train_samples_per_class\"] * data[\"metadata\"][\"num_shared_classes\"]\n",
    "\n",
    "num_classes_per_task = data[\"metadata\"][\"num_shared_classes\"] + data[\"metadata\"][\"num_novel_classes_per_task\"]\n",
    "num_train_samples_per_task = data[\"metadata\"][\"num_train_samples_per_class\"] * num_classes_per_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the local labels back to global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(1, num_tasks + 1):\n",
    "\n",
    "    global_to_local_map = data[\"metadata\"][\"global_to_local_class_mappings\"][f\"task_{task_ind}\"]\n",
    "    local_to_global_map = {v: int(k) for k, v in global_to_local_map.items()}\n",
    "\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "        data[f\"task_{task_ind}_{mode}\"] = data[f\"task_{task_ind}_{mode}\"].map(\n",
    "            lambda row: {\"y\": local_to_global_map[row[\"y\"].item()]}\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anchors = data[\"task_0_train\"][\"embedding\"].shape[1]\n",
    "print(f\"Using {num_anchors} anchors\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get shared samples indices\n",
    "Add **shared** column, `True` for samples belonging to shared classes and False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(num_tasks + 1):\n",
    "\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "\n",
    "        data[f\"task_{task_ind}_{mode}\"] = data[f\"task_{task_ind}_{mode}\"].map(\n",
    "            lambda row: {\"shared\": row[\"y\"].item() in shared_classes}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_ids = []\n",
    "\n",
    "for task_ind in range(num_tasks + 1):\n",
    "    all_ids = data[f\"task_{task_ind}_train\"][\"id\"]\n",
    "\n",
    "    # get the indices of samples having shared to True\n",
    "    task_shared_ids = all_ids[data[f\"task_{task_ind}_train\"][\"shared\"]].tolist()\n",
    "\n",
    "    shared_ids.append(task_shared_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the shared indices are the same across all the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks + 1):\n",
    "    for task_j in range(task_i, num_tasks + 1):\n",
    "        assert shared_ids[task_i] == shared_ids[task_j]\n",
    "\n",
    "shared_ids = shared_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample anchor indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_ids = random.sample(shared_ids, num_anchors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add **anchor** column, being `True` only if the corresponding sample is an anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only training samples can be anchors\n",
    "for task_ind in range(num_tasks + 1):\n",
    "    data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "        lambda row: {\"anchor\": row[\"id\"].item() in anchor_ids}\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) center the spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centering = False\n",
    "if centering:\n",
    "    for task_ind in range(num_tasks + 1):\n",
    "        embedding_mean = data[f\"task_{task_ind}_train\"][\"embedding\"].mean(dim=0)\n",
    "        data[f\"task_{task_ind}_train\"] = data[f\"task_{task_ind}_train\"].map(\n",
    "            lambda row: {\"embedding\": row[\"embedding\"] - embedding_mean}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map to relative spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "\n",
    "    task_anchors = data[f\"task_{task_ind}_train\"][\"embedding\"][data[f\"task_{task_ind}_train\"][\"anchor\"]]\n",
    "    norm_anchors = F.normalize(task_anchors, p=2, dim=-1)\n",
    "\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "\n",
    "        task_embeddings = data[f\"task_{task_ind}_{mode}\"][\"embedding\"]\n",
    "\n",
    "        abs_space = F.normalize(task_embeddings, p=2, dim=-1)\n",
    "\n",
    "        rel_space = abs_space @ norm_anchors.T\n",
    "\n",
    "        data[f\"task_{task_ind}_{mode}\"] = add_tensor_column(\n",
    "            data[f\"task_{task_ind}_{mode}\"], \"relative_embeddings\", rel_space\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_ind in range(0, num_tasks + 1):\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "        data[f\"task_{task_ind}_{mode}\"].set_format(\n",
    "            type=\"torch\", columns=[\"relative_embeddings\", \"embedding\", \"y\", \"id\", \"shared\"]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the shared and the non-shared samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_samples = {\"train\": [], \"test\": []}\n",
    "disjoint_samples = {\"train\": [], \"test\": []}\n",
    "\n",
    "for task_ind in range(1, num_tasks + 1):\n",
    "\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "\n",
    "        task_shared_samples = data[f\"task_{task_ind}_{mode}\"].filter(lambda row: row[\"shared\"]).sort(\"id\")\n",
    "\n",
    "        task_novel_samples = data[f\"task_{task_ind}_{mode}\"].filter(lambda row: ~row[\"shared\"])\n",
    "\n",
    "        shared_samples[mode].append(task_shared_samples)\n",
    "        disjoint_samples[mode].append(task_novel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_samples = {\n",
    "    \"train\": concatenate_datasets(disjoint_samples[\"train\"]),\n",
    "    \"test\": concatenate_datasets(disjoint_samples[\"test\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average the shared samples and then concat the task-specific samples and the shared samples to go to the merged space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of the shared_samples and put them back in the dataset\n",
    "# Extract the 'embedding' columns from each dataset\n",
    "\n",
    "merged_datasets = {\"train\": [], \"test\": []}\n",
    "\n",
    "for mode in [\"train\", \"test\"]:\n",
    "    shared_rel_embeddings = [dataset[\"relative_embeddings\"] for dataset in shared_samples[mode]]\n",
    "\n",
    "    # Calculate the mean of the embeddings for each sample\n",
    "    mean_embeddings = torch.mean(torch.stack(shared_rel_embeddings), dim=0)\n",
    "\n",
    "    # Create a new dataset with the same features as the original datasets\n",
    "    new_features = shared_samples[mode][0].features.copy()\n",
    "\n",
    "    # Replace the 'embedding' column in the new dataset with the mean embeddings\n",
    "    new_data = {column: shared_samples[mode][0][column] for column in new_features}\n",
    "    new_data[\"relative_embeddings\"] = mean_embeddings.tolist()\n",
    "\n",
    "    # Create the new Hugging Face dataset\n",
    "    shared_dataset = Dataset.from_dict(new_data, features=new_features)\n",
    "\n",
    "    merged_dataset = concatenate_datasets([shared_dataset, novel_samples[mode]])\n",
    "\n",
    "    merged_datasets[mode] = merged_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort both datasets by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_coarse_label = dataset_name == \"cifar_100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"test\"  # train or test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = merged_datasets[mode].sort(\"id\")\n",
    "original_dataset = data[f\"task_0_{mode}\"].sort(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"relative_embeddings\", \"y\", \"embedding\"]\n",
    "if has_coarse_label:\n",
    "    columns.append(\"coarse_label\")\n",
    "\n",
    "merged_dataset.set_format(type=\"torch\", columns=columns)\n",
    "original_dataset.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "merged_space = merged_dataset[\"relative_embeddings\"]\n",
    "original_space = original_dataset[\"relative_embeddings\"]\n",
    "\n",
    "original_space_y = original_dataset[\"y\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.cka import CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cka = CKA(mode=\"linear\", device=\"cuda\")\n",
    "\n",
    "cka_score = cka(merged_space, original_space)\n",
    "print(cka_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.relative_analysis import plot_space_grid\n",
    "from la.utils.relative_analysis import plot_pairwise_dist\n",
    "from la.utils.relative_analysis import plot_self_dist\n",
    "from la.utils.relative_analysis import Reduction, reduce\n",
    "from la.utils.relative_analysis import self_sim_comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole space (all classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_dim: int = 1000\n",
    "subsample_indices = random.sample(range(0, original_space.shape[0]), subsample_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_original = original_space[subsample_indices]\n",
    "subsample_merged = merged_space[subsample_indices]\n",
    "subsample_labels = original_space_y[subsample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices: torch.Tensor = subsample_labels.sort().indices\n",
    "\n",
    "subsample_original_sorted: torch.Tensor = subsample_original[sort_indices]\n",
    "subsample_merged_sorted: torch.Tensor = subsample_merged[sort_indices]\n",
    "subsample_labels_sorted: torch.Tensor = subsample_labels[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_sim_comparison(space1=subsample_original_sorted, space2=subsample_merged_sorted, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_self_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_header = [reduction.upper() for reduction in Reduction]\n",
    "y_header = [\"Relative Space 1\", \"Relative Space 2\"]\n",
    "\n",
    "spaces = [\n",
    "    [\n",
    "        *reduce(space1=subsample_original_sorted, space2=subsample_merged_sorted, reduction=reduction),\n",
    "    ]\n",
    "    for reduction in Reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_coarse_label:\n",
    "    original_space_coarse_labels = original_dataset[\"coarse_label\"]\n",
    "    subsample_coarse_labels = original_space_coarse_labels[subsample_indices]\n",
    "    subsample_coarse_labels_sorted: torch.Tensor = subsample_coarse_labels[sort_indices]\n",
    "    fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_coarse_labels_sorted)\n",
    "    fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only non-shared classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_nonshared = merged_dataset.filter(lambda row: row[\"y\"].item() in non_shared_classes)\n",
    "original_dataset_nonshared = original_dataset.filter(lambda row: row[\"y\"].item() in non_shared_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space_nonshared = merged_dataset_nonshared[\"relative_embeddings\"]\n",
    "original_space_nonshared = original_dataset_nonshared[\"relative_embeddings\"]\n",
    "original_space_y_nonshared = original_dataset_nonshared[\"y\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_dim: int = 1000\n",
    "subsample_indices = random.sample(range(0, original_space_nonshared.shape[0]), subsample_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_original = original_space_nonshared[subsample_indices]\n",
    "subsample_merged = merged_space_nonshared[subsample_indices]\n",
    "subsample_labels = original_space_y_nonshared[subsample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices: torch.Tensor = subsample_labels.sort().indices\n",
    "\n",
    "subsample_original_sorted: torch.Tensor = subsample_original[sort_indices]\n",
    "subsample_merged_sorted: torch.Tensor = subsample_merged[sort_indices]\n",
    "subsample_labels_sorted: torch.Tensor = subsample_labels[sort_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_sim_comparison(space1=subsample_original_sorted, space2=subsample_merged_sorted, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_self_dist(space1=subsample_original_sorted, space2=subsample_merged_sorted, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_header = [reduction.upper() for reduction in Reduction]\n",
    "y_header = [\"Relative Space 1\", \"Relative Space 2\"]\n",
    "\n",
    "spaces = [\n",
    "    [\n",
    "        *reduce(space1=subsample_original_sorted, space2=subsample_merged_sorted, reduction=reduction),\n",
    "    ]\n",
    "    for reduction in Reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_labels_sorted)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_coarse_label:\n",
    "    original_space_coarse_labels_nonshared = original_dataset_nonshared[\"coarse_label\"]\n",
    "    subsample_coarse_labels = original_space_coarse_labels_nonshared[subsample_indices]\n",
    "    subsample_coarse_labels_sorted: torch.Tensor = subsample_coarse_labels[sort_indices]\n",
    "    fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=subsample_coarse_labels_sorted)\n",
    "    fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color by task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_novel_classes(task_classes, shared_classes):\n",
    "    return set(task_classes).difference(shared_classes)\n",
    "\n",
    "\n",
    "def get_label_to_task_mapping():\n",
    "\n",
    "    novel_classes_per_task = []\n",
    "    for i in range(1, num_tasks + 1):\n",
    "        task_classes = [int(key) for key in data[\"metadata\"][\"global_to_local_class_mappings\"][f\"task_{i}\"].keys()]\n",
    "        task_novel_classes = get_novel_classes(task_classes, shared_classes)\n",
    "\n",
    "        assert len(task_novel_classes) == num_novel_classes\n",
    "\n",
    "        novel_classes_per_task.append(task_novel_classes)\n",
    "\n",
    "    label_to_task = {}\n",
    "    for task_ind in range(0, num_tasks):\n",
    "        for label in novel_classes_per_task[task_ind]:\n",
    "            label_to_task[label] = task_ind\n",
    "\n",
    "    return label_to_task\n",
    "\n",
    "\n",
    "label_to_task = get_label_to_task_mapping()\n",
    "print(label_to_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_labels = subsample_labels_sorted.apply_(lambda x: label_to_task[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=task_labels)\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only non-shared classes, prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_space_nonshared = merged_dataset_nonshared[\"relative_embeddings\"]\n",
    "original_space_nonshared = original_dataset_nonshared[\"relative_embeddings\"]\n",
    "original_space_y_nonshared = original_dataset_nonshared[\"y\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prototypes_merged = []\n",
    "for class_ind in non_shared_classes:\n",
    "    class_prototypes_merged.append(torch.mean(merged_space_nonshared[original_space_y_nonshared == class_ind], dim=0))\n",
    "\n",
    "class_prototypes_merged = torch.stack(class_prototypes_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prototypes_original = []\n",
    "for class_ind in non_shared_classes:\n",
    "    class_prototypes_original.append(\n",
    "        torch.mean(original_space_nonshared[original_space_y_nonshared == class_ind], dim=0)\n",
    "    )\n",
    "\n",
    "class_prototypes_original = torch.stack(class_prototypes_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_dist(space1=class_prototypes_original, space2=class_prototypes_merged, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_self_dist(space1=class_prototypes_original, space2=class_prototypes_merged, prefix=\"Relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_header = [reduction.upper() for reduction in Reduction]\n",
    "y_header = [\"Relative Space 1\", \"Relative Space 2\"]\n",
    "\n",
    "spaces = [\n",
    "    [\n",
    "        *reduce(space1=class_prototypes_original, space2=class_prototypes_merged, reduction=reduction, perplexity=15),\n",
    "    ]\n",
    "    for reduction in Reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_space_grid(x_header=x_header, y_header=y_header, spaces=spaces, c=np.arange(len(non_shared_classes)))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import RichProgressBar, TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "dataloader_func = partial(\n",
    "    torch.utils.data.DataLoader,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "trainer_func = partial(Trainer, gpus=1, max_epochs=100, logger=False, enable_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_embed_dim = 256\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, classifier_embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=original_space.shape[1]),\n",
    "            nn.Linear(input_dim, classifier_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(classifier_embed_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, classifier: nn.Module, shared_classes: set, non_shared_classes: set, use_relatives: bool):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "\n",
    "        shared_classes = torch.Tensor(list(shared_classes)).long()\n",
    "        non_shared_classes = torch.Tensor(list(non_shared_classes)).long()\n",
    "\n",
    "        self.register_buffer(\"shared_classes\", shared_classes)\n",
    "        self.register_buffer(\"non_shared_classes\", non_shared_classes)\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "        self.use_relatives = use_relatives\n",
    "        self.embedding_key = \"relative_embeddings\" if self.use_relatives else \"embedding\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch[self.embedding_key], batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[self.embedding_key], batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_step=True, prog_bar=True)\n",
    "\n",
    "        val_acc = self.accuracy(y_hat, y)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch[self.embedding_key], batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True)\n",
    "\n",
    "        test_acc = self.accuracy(y_hat, y)\n",
    "        self.log(\"test_acc\", test_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # compute accuracy for shared classes\n",
    "        shared_classes_mask = torch.isin(y, self.shared_classes)\n",
    "        shared_classes_y = y[shared_classes_mask]\n",
    "\n",
    "        y_hat = torch.argmax(y_hat, dim=1)\n",
    "        shared_classes_y_hat = y_hat[shared_classes_mask]\n",
    "\n",
    "        shared_classes_acc = torch.sum(shared_classes_y == shared_classes_y_hat) / len(shared_classes_y)\n",
    "        self.log(\"test_acc_shared_classes\", shared_classes_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # compute accuracy for non-shared classes\n",
    "        non_shared_classes_mask = torch.isin(y, self.non_shared_classes)\n",
    "        non_shared_classes_y = y[non_shared_classes_mask]\n",
    "        non_shared_classes_y_hat = y_hat[non_shared_classes_mask]\n",
    "\n",
    "        non_shared_classes_acc = torch.sum(non_shared_classes_y == non_shared_classes_y_hat) / len(non_shared_classes_y)\n",
    "        self.log(\"test_acc_non_shared_classes\", non_shared_classes_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier over all the classes, original, absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_classifier = Classifier(\n",
    "    input_dim=original_space.shape[1], classifier_embed_dim=classifier_embed_dim, num_classes=num_total_classes\n",
    ")\n",
    "original_model = Model(\n",
    "    classifier=original_classifier,\n",
    "    shared_classes=shared_classes,\n",
    "    non_shared_classes=non_shared_classes,\n",
    "    use_relatives=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_func(callbacks=[pytorch_lightning.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in train, val and test\n",
    "split_dataset = original_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "original_dataset_train = split_dataset[\"train\"]\n",
    "original_dataset_val_test = split_dataset[\"test\"]\n",
    "\n",
    "split_val_test = original_dataset_val_test.train_test_split(test_size=0.5, seed=42)\n",
    "original_dataset_val = split_val_test[\"train\"]\n",
    "original_dataset_test = split_val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_dataloader = dataloader_func(original_dataset_train, shuffle=True)\n",
    "original_val_dataloader = dataloader_func(original_dataset_val, shuffle=False)\n",
    "original_test_dataloader = dataloader_func(original_dataset_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(original_model, original_train_dataloader, original_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(original_model, original_test_dataloader)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification over all the classes, original, relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_classifier = Classifier(\n",
    "    input_dim=original_space.shape[1], classifier_embed_dim=classifier_embed_dim, num_classes=num_total_classes\n",
    ")\n",
    "original_model = Model(\n",
    "    classifier=original_classifier,\n",
    "    shared_classes=shared_classes,\n",
    "    non_shared_classes=non_shared_classes,\n",
    "    use_relatives=True,\n",
    ")\n",
    "trainer = trainer_func(callbacks=[pytorch_lightning.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in train, val and test\n",
    "split_dataset = original_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "original_dataset_train = split_dataset[\"train\"]\n",
    "original_dataset_val_test = split_dataset[\"test\"]\n",
    "\n",
    "split_val_test = original_dataset_val_test.train_test_split(test_size=0.5, seed=42)\n",
    "original_dataset_val = split_val_test[\"train\"]\n",
    "original_dataset_test = split_val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_dataloader = dataloader_func(original_dataset_train, shuffle=True)\n",
    "original_val_dataloader = dataloader_func(original_dataset_val, shuffle=False)\n",
    "original_test_dataloader = dataloader_func(original_dataset_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(original_model, original_train_dataloader, original_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(original_model, original_test_dataloader)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification over all the classes, merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in train, val and test\n",
    "split_dataset = merged_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "merged_dataset_train = split_dataset[\"train\"]\n",
    "merged_dataset_val_test = split_dataset[\"test\"]\n",
    "\n",
    "split_val_test = merged_dataset_val_test.train_test_split(test_size=0.5, seed=42)\n",
    "merged_dataset_val = split_val_test[\"train\"]\n",
    "merged_dataset_test = split_val_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_dataloader = dataloader_func(merged_dataset_train, shuffle=True)\n",
    "merged_val_dataloader = dataloader_func(merged_dataset_val, shuffle=False)\n",
    "merged_test_dataloader = dataloader_func(merged_dataset_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_classifier = Classifier(\n",
    "    input_dim=merged_space.shape[1], classifier_embed_dim=classifier_embed_dim, num_classes=num_total_classes\n",
    ")\n",
    "merged_model = Model(\n",
    "    classifier=merged_classifier,\n",
    "    shared_classes=shared_classes,\n",
    "    non_shared_classes=non_shared_classes,\n",
    "    use_relatives=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer_func(callbacks=[pytorch_lightning.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)])\n",
    "trainer.fit(merged_model, merged_train_dataloader, merged_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(merged_model, merged_test_dataloader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
