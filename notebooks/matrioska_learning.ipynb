{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "from la.data.my_dataset_dict import MyDatasetDict\n",
    "\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"matrioska_learning\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_core.common import PROJECT_ROOT\n",
    "\n",
    "# Instantiate torchvision dataset\n",
    "cfg = compose(config_name=\"matrioska_learning\", overrides=[])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.io_utils import add_ids_to_dataset, load_data\n",
    "from la.utils.io_utils import preprocess_dataset\n",
    "\n",
    "\n",
    "original_dataset = dataset = load_data(cfg)  # .shard(num_shards=10, index=0)  # TODO remove sharding when done develop\n",
    "dataset = preprocess_dataset(dataset, cfg)\n",
    "dataset = add_ids_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = dataset[\"train\"][0][\"x\"].shape[1]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf specific variables\n",
    "# (if a dataset change is needed, it is enough to redefine these variables...)\n",
    "class_names = original_dataset[\"train\"].features[\"fine_label\"].names\n",
    "class_idxs = [original_dataset[\"train\"].features[\"fine_label\"].str2int(class_name) for class_name in class_names]\n",
    "\n",
    "class_names, class_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Result:\n",
    "    matrioska_idx: int\n",
    "    num_train_classes: int\n",
    "    metric_name: str\n",
    "    score: float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define matrioska datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrioska parameters... just start with the first two classes\n",
    "MATRIOSKA_START_NCLASSES = [0, 1]\n",
    "LIMIT_N_CLASSES = 30\n",
    "remanining_classes = sorted((set(class_idxs) - set(MATRIOSKA_START_NCLASSES)))[:LIMIT_N_CLASSES]\n",
    "MATRIOSKA_START_NCLASSES, remanining_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate matrioska classes\n",
    "matrioskaclasses = [set(MATRIOSKA_START_NCLASSES + remanining_classes[:i]) for i in range(len(remanining_classes) + 1)]\n",
    "matrioskaclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate associated datasets\n",
    "# TODO: do we want to have the same number of samples in all the datasets?\n",
    "# I think not. This is more fair, if this works we are in the worst case scenario.\n",
    "matrioskaidx2dataset = {\n",
    "    i: dataset.filter(lambda row: row[\"y\"] in matrioskaclasses[i]) for i in range(len(matrioskaclasses))\n",
    "}\n",
    "\n",
    "# Note that we are using the prefix convention for the classes, thus we have consistency\n",
    "# between local and global classes ids... let's stay with that it is easier\n",
    "matrioskaidx2dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train matrioska models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from typing import Dict\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "matrioskaidx2dataset\n",
    "\n",
    "matrioskaidx2embeds: Dict[str, DatasetDict] = {\n",
    "    f\"matrioska{matrioska_idx}\": DatasetDict(train=DatasetDict(), test=DatasetDict())\n",
    "    for matrioska_idx in range(len(matrioskaclasses))\n",
    "}\n",
    "len(matrioskaidx2embeds), matrioskaidx2embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "HF_EMBEDDING_DATASET_PATH = PROJECT_ROOT / \"matrioska_learning\" / \"hf_embedding_dataset\"\n",
    "\n",
    "\n",
    "def embed_and_save_samples(matrioskaidx2embeds, dataset, model, matrioska_idx, batch_size=1024) -> Dict:\n",
    "    modes = [\"train\", \"test\"]\n",
    "\n",
    "    model.cuda().eval()\n",
    "\n",
    "    for mode in modes:\n",
    "        mode_embeddings = []\n",
    "        mode_ids = []\n",
    "        mode_labels = []\n",
    "        mode_loader = DataLoader(\n",
    "            dataset[mode],\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=True,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        for batch in tqdm(mode_loader, desc=f\"Embedding {mode} samples for {matrioska_idx}th matrioska\"):\n",
    "            x = batch[\"x\"].to(\"cuda\")\n",
    "            mode_embeddings.extend(model(x)[\"embeds\"].detach())\n",
    "            mode_ids.extend(batch[\"id\"])\n",
    "            mode_labels.extend(batch[\"y\"])\n",
    "\n",
    "        matrioskaidx2embeds[f\"matrioska{matrioska_idx}\"][mode] = Dataset.from_dict(\n",
    "            {\n",
    "                \"embeds\": mode_embeddings,\n",
    "                \"id\": mode_ids,\n",
    "                \"y\": mode_labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    model.cpu()\n",
    "    matrioskaidx2embeds[f\"matrioska{matrioska_idx}\"].save_to_disk(\n",
    "        HF_EMBEDDING_DATASET_PATH / f\"matrioska{matrioska_idx}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.model_logging import NNLogger\n",
    "from nn_core.serialization import NNCheckpointIO\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Callback\n",
    "\n",
    "from la.utils.utils import build_callbacks\n",
    "\n",
    "\n",
    "matrioskaidx2model = {}\n",
    "\n",
    "\n",
    "for i in range(len(matrioskaclasses)):\n",
    "    print(f\"Training model {i}...\")\n",
    "\n",
    "    model: pl.LightningModule = hydra.utils.instantiate(\n",
    "        cfg.nn.model,\n",
    "        _recursive_=False,\n",
    "        num_classes=len(matrioskaclasses[i]),\n",
    "        model=cfg.nn.model.model,\n",
    "        input_dim=img_size,\n",
    "    )\n",
    "\n",
    "    processed_dataset = matrioskaidx2dataset[i].map(\n",
    "        desc=f\"Preprocessing samples\",\n",
    "        function=lambda x: {\"x\": model.transform_func(x[\"x\"])},\n",
    "    )\n",
    "    processed_dataset.set_format(type=\"torch\", columns=[\"x\", \"y\", \"id\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        processed_dataset[\"train\"],\n",
    "        batch_size=512,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        processed_dataset[\"test\"],\n",
    "        batch_size=512,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "    )\n",
    "\n",
    "    template_core: NNTemplateCore = NNTemplateCore(\n",
    "        restore_cfg=cfg.train.get(\"restore\", None),\n",
    "    )\n",
    "    callbacks: List[Callback] = build_callbacks(cfg.train.callbacks, template_core)\n",
    "\n",
    "    storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "    logger: NNLogger = NNLogger(logging_cfg=cfg.train.logging, cfg=cfg, resume_id=template_core.resume_id)\n",
    "\n",
    "    # Use this in case we need to restore models, search for it in the wandb UI\n",
    "    logger.experiment.config[\"matrioska_idx\"] = i\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=storage_dir,\n",
    "        plugins=[NNCheckpointIO(jailing_dir=logger.run_dir)],\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        **cfg.train.trainer,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    matrioskaidx2model[i] = trainer.model.eval().cpu().requires_grad_(False)\n",
    "\n",
    "    embed_and_save_samples(matrioskaidx2embeds, processed_dataset, matrioskaidx2model[i], i, batch_size=1024)\n",
    "    logger.experiment.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalute matrioska models with classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "\n",
    "HF_EMBEDDING_DATASET_PATH = PROJECT_ROOT / \"matrioska_learning\" / \"hf_embedding_dataset\"\n",
    "N_MATRIOSKA = 21\n",
    "\n",
    "matrioskaidx2embeds = {\n",
    "    i: datasets.load_from_disk(HF_EMBEDDING_DATASET_PATH / f\"matrioska{i}\") for i in range(N_MATRIOSKA)\n",
    "}\n",
    "len(matrioskaidx2embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which classes to evaluate on -- it may be interesting to change this\n",
    "EVALUATION_CLASSES = {0, 1, 2, 3, 4}\n",
    "EVALUATION_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.model_logging import NNLogger\n",
    "from nn_core.serialization import NNCheckpointIO\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Callback\n",
    "from la.pl_modules.classifier import Classifier\n",
    "\n",
    "from la.utils.utils import build_callbacks\n",
    "\n",
    "performance = []\n",
    "\n",
    "# Iterate over models that have been trained on at least EVALUATION_CLASSES\n",
    "for matrioska_idx, embeds in list(matrioskaidx2embeds.items())[len(EVALUATION_CLASSES) - 2 :]:\n",
    "    embeds_dataset = matrioskaidx2embeds[matrioska_idx].filter(\n",
    "        lambda x: x[\"y\"] in EVALUATION_CLASSES,\n",
    "    )\n",
    "    embeds_dataset.set_format(type=\"torch\", columns=[\"embeds\", \"y\"])\n",
    "\n",
    "    eval_train_loader = DataLoader(\n",
    "        embeds_dataset[\"train\"],\n",
    "        batch_size=64,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    eval_test_loader = DataLoader(\n",
    "        embeds_dataset[\"test\"],\n",
    "        batch_size=64,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    model = Classifier(\n",
    "        input_dim=embeds_dataset[\"train\"][\"embeds\"].size(1),\n",
    "        num_classes=len(EVALUATION_CLASSES),\n",
    "        lr=1e-4,\n",
    "        deep=True,\n",
    "        x_feature=\"embeds\",\n",
    "        y_feature=\"y\",\n",
    "    )\n",
    "\n",
    "    callbacks: List[Callback] = build_callbacks(cfg.train.callbacks)\n",
    "\n",
    "    storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=storage_dir,\n",
    "        logger=None,\n",
    "        fast_dev_run=False,\n",
    "        gpus=1,\n",
    "        precision=32,\n",
    "        max_epochs=50,\n",
    "        accumulate_grad_batches=1,\n",
    "        num_sanity_val_steps=2,\n",
    "        gradient_clip_val=10.0,\n",
    "        val_check_interval=1.0,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=eval_train_loader, val_dataloaders=eval_test_loader)\n",
    "\n",
    "    classifier_model = trainer.model.eval().cpu().requires_grad_(False)\n",
    "    run_results = trainer.test(model=classifier_model, dataloaders=eval_test_loader)[0]\n",
    "\n",
    "    performance.extend(\n",
    "        (\n",
    "            Result(\n",
    "                matrioska_idx=matrioska_idx,\n",
    "                num_train_classes=len(matrioskaclasses[matrioska_idx]),\n",
    "                metric_name=\"test_accuracy\",\n",
    "                score=run_results[\"accuracy\"],\n",
    "            ),\n",
    "            Result(\n",
    "                matrioska_idx=matrioska_idx,\n",
    "                num_train_classes=len(matrioskaclasses[matrioska_idx]),\n",
    "                metric_name=\"test_f1\",\n",
    "                score=run_results[\"f1\"],\n",
    "            ),\n",
    "            Result(\n",
    "                matrioska_idx=matrioska_idx,\n",
    "                num_train_classes=len(matrioskaclasses[matrioska_idx]),\n",
    "                metric_name=\"test_loss\",\n",
    "                score=run_results[\"test_loss\"],\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "perf = pd.DataFrame(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.scatter(perf, x=\"matrioska_idx\", y=\"score\", color=\"metric_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~Evaluate matrioska models with clusters~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Deprectaed!\n",
    "# # This experiments is deprecated:\n",
    "# #  - Clustering metrics are not informative\n",
    "# #  - The current code requires having the models in memory (can be easily adapted to load from disk the hf dataset)\n",
    "\n",
    "\n",
    "# # Decide which classes to evaluate on -- it may be interesting to change this\n",
    "# EVALUATION_CLASSES = MATRIOSKA_START_NCLASSES\n",
    "# EVALUATION_CLASSES\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Define the evaluation dataset according to chosen classes\n",
    "# eval_dataset = dataset.filter(lambda row: row[\"y\"] in set(EVALUATION_CLASSES))\n",
    "# eval_dataset = eval_dataset.map(\n",
    "#     desc=f\"Preprocessing samples\",\n",
    "#     function=lambda x: {\"x\": model.transform_func(x[\"x\"])},\n",
    "# )\n",
    "# eval_dataset.set_format(type=\"torch\", columns=[\"x\", \"y\"])\n",
    "\n",
    "# eval_train_loader = DataLoader(\n",
    "#     eval_dataset[\"test\"],\n",
    "#     batch_size=64,\n",
    "#     pin_memory=True,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,\n",
    "# )\n",
    "\n",
    "# eval_test_loader = DataLoader(\n",
    "#     eval_dataset[\"test\"],\n",
    "#     batch_size=64,\n",
    "#     pin_memory=True,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     accelerator=\"auto\",\n",
    "#     devices=1,\n",
    "#     max_epochs=3,\n",
    "#     logger=None,\n",
    "#     # callbacks=[RichProgressBar()],\n",
    "#     enable_progress_bar=True,\n",
    "# )\n",
    "\n",
    "# eval_dataset\n",
    "\n",
    "# import dataclasses\n",
    "\n",
    "\n",
    "# @dataclasses.dataclass\n",
    "# class Result:\n",
    "#     matrioska_idx: int\n",
    "#     clusterer: str\n",
    "#     metric_name: str\n",
    "#     score: float\n",
    "\n",
    "#     from sklearn.cluster import KMeans, BisectingKMeans\n",
    "# import sklearn\n",
    "# import torch\n",
    "\n",
    "\n",
    "# model = matrioskaidx2model[0]\n",
    "\n",
    "\n",
    "# def compute_eval_embedings(model, eval_loader):\n",
    "#     eval_embeddings = []\n",
    "#     eval_labels = []\n",
    "#     for batch in eval_loader:\n",
    "#         out = model(batch[\"x\"])\n",
    "#         eval_embeddings.append(out[\"embeds\"])\n",
    "#         eval_labels.append(batch[\"y\"])\n",
    "\n",
    "#     eval_embeddings = torch.cat(eval_embeddings, dim=0)\n",
    "#     eval_labels = torch.cat(eval_labels, dim=0)\n",
    "#     return eval_embeddings.detach().cpu().numpy(), eval_labels.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# clusterizer = {\n",
    "#     \"kmeans\": lambda embeds: KMeans(n_clusters=len(EVALUATION_CLASSES)).fit(embeds).labels_,\n",
    "#     \"bisect-kmeans\": lambda embeds: BisectingKMeans(n_clusters=len(EVALUATION_CLASSES)).fit(embeds).labels_,\n",
    "#     \"dbscan\": lambda embeds: sklearn.cluster.DBSCAN().fit(embeds).labels_,\n",
    "#     \"spectral\": lambda embeds: sklearn.cluster.SpectralClustering(n_clusters=len(EVALUATION_CLASSES))\n",
    "#     .fit(embeds)\n",
    "#     .labels_,\n",
    "#     \"birch\": lambda embeds: sklearn.cluster.Birch(n_clusters=len(EVALUATION_CLASSES)).fit(embeds).labels_,\n",
    "#     \"agglomerative\": lambda embeds: sklearn.cluster.AgglomerativeClustering(n_clusters=len(EVALUATION_CLASSES))\n",
    "#     .fit(embeds)\n",
    "#     .labels_,\n",
    "#     \"optics\": lambda embeds: sklearn.cluster.OPTICS().fit(embeds).labels_,\n",
    "# }\n",
    "\n",
    "# clustering_metric = {\n",
    "#     \"v_measure_score\": lambda x, y_pred, y_true: sklearn.metrics.v_measure_score(y_true, y_pred),\n",
    "#     \"adjusted_mutual_info_score\": lambda x, y_pred, y_true: sklearn.metrics.adjusted_mutual_info_score(y_true, y_pred),\n",
    "#     \"adjusted_rand_score\": lambda x, y_pred, y_true: sklearn.metrics.adjusted_rand_score(y_true, y_pred),\n",
    "#     \"completeness_score\": lambda x, y_pred, y_true: sklearn.metrics.completeness_score(y_true, y_pred),\n",
    "#     \"fowlkes_mallows_score\": lambda x, y_pred, y_true: sklearn.metrics.fowlkes_mallows_score(y_true, y_pred),\n",
    "#     # \"homogeneity_completeness_v_measure\": lambda x, y_pred, y_true: sklearn.metrics.homogeneity_completeness_v_measure(\n",
    "#     # y_true, y_pred\n",
    "#     # ),\n",
    "#     \"homogeneity_score\": lambda x, y_pred, y_true: sklearn.metrics.homogeneity_score(y_true, y_pred),\n",
    "#     \"mutual_info_score\": lambda x, y_pred, y_true: sklearn.metrics.mutual_info_score(y_true, y_pred),\n",
    "#     \"normalized_mutual_info_score\": lambda x, y_pred, y_true: sklearn.metrics.normalized_mutual_info_score(\n",
    "#         y_true, y_pred\n",
    "#     ),\n",
    "#     \"rand_score\": lambda x, y_pred, y_true: sklearn.metrics.rand_score(y_true, y_pred),\n",
    "# }\n",
    "\n",
    "# performance = []\n",
    "# for i in range(len(matrioskaidx2model)):\n",
    "#     result = trainer.test(model=matrioskaidx2model[i], dataloaders=eval_loader)[0]\n",
    "\n",
    "#     performance.append(Result(matrioska_idx=i, clusterer=\"none\", metric_name=\"test_acc\", score=result[\"acc/test\"]))\n",
    "#     performance.append(Result(matrioska_idx=i, clusterer=\"none\", metric_name=\"test_loss\", score=result[\"loss/test\"]))\n",
    "#     eval_embeddings, eval_labels = compute_eval_embedings(model, eval_loader)\n",
    "\n",
    "#     for clusterizer_name, clusterizer_func in clusterizer.items():\n",
    "#         clustering_labels = clusterizer[clusterizer_name](eval_embeddings)\n",
    "\n",
    "#         for metric_name, metric_func in clustering_metric.items():\n",
    "#             performance.append(\n",
    "#                 Result(\n",
    "#                     matrioska_idx=i,\n",
    "#                     clusterer=clusterizer_name,\n",
    "#                     metric_name=metric_name,\n",
    "#                     score=metric_func(x=eval_embeddings, y_pred=clustering_labels, y_true=eval_labels),\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "\n",
    "# perf = pd.DataFrame(performance)\n",
    "# perf\n",
    "# perf[\"ntrain_classes\"] = perf[\"matrioska_idx\"] + 2\n",
    "\n",
    "# fig = px.scatter(\n",
    "#     perf,\n",
    "#     facet_col=\"clusterer\",\n",
    "#     facet_row=\"metric_name\",\n",
    "#     x=\"ntrain_classes\",\n",
    "#     y=\"score\",\n",
    "#     labels={\"matrioska_idx\": \"Number of classes trained on\", \"test_acc\": \"Test accuracy\"},\n",
    "#     height=2000,\n",
    "# )\n",
    "# fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying K classes in the latent space of N classes or K<N classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.model_logging import NNLogger\n",
    "from nn_core.serialization import NNCheckpointIO\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Callback\n",
    "from la.utils.utils import build_callbacks\n",
    "from torch.utils.data import DataLoader\n",
    "from la.data.my_dataset_dict import MyDatasetDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_task_classes = 10\n",
    "num_tasks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Task:\n",
    "    class_idxs: list\n",
    "    classes: list\n",
    "    global_to_local: Dict\n",
    "    id: str\n",
    "    dataset: Union[DatasetDict, MyDatasetDict] = None\n",
    "    embedded_dataset: MyDatasetDict = None\n",
    "    model: pl.LightningModule = None\n",
    "\n",
    "    def metadata(self):\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"class_idxs\": self.class_idxs,\n",
    "            \"classes\": self.classes,\n",
    "            \"global_to_local\": self.global_to_local,\n",
    "            \"model\": self.model,\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task creation or loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_tasks = True\n",
    "\n",
    "SUBTASK_DATASET_PATH = PROJECT_ROOT / \"matrioska_learning\" / \"subtasks\"\n",
    "SUBTASK_DATASET_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not create_new_tasks:\n",
    "\n",
    "    tasks = []\n",
    "    for dataset_path in SUBTASK_DATASET_PATH.glob(\"*\"):\n",
    "        dataset = MyDatasetDict.load_from_disk(dataset_path)\n",
    "        task = Task(\n",
    "            id=dataset_path.name,\n",
    "            class_idxs=dataset[\"metadata\"][\"class_idxs\"],\n",
    "            classes=dataset[\"metadata\"][\"classes\"],\n",
    "            global_to_local=dataset[\"metadata\"][\"global_to_local\"],\n",
    "            dataset=dataset,\n",
    "        )\n",
    "\n",
    "        tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_new_tasks:\n",
    "    tasks = []\n",
    "\n",
    "    all_classes_task = Task(\n",
    "        class_idxs=class_idxs,\n",
    "        classes=class_names,\n",
    "        global_to_local={i: i for i in range(len(class_names))},\n",
    "        id=\"all_classes\",\n",
    "        dataset=MyDatasetDict(train=dataset[\"train\"], test=dataset[\"test\"]),\n",
    "        embedded_dataset=MyDatasetDict(train=DatasetDict(), test=DatasetDict()),\n",
    "    )\n",
    "\n",
    "    tasks.append(all_classes_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate\n",
    "\n",
    "transform_func = instantiate(cfg.nn.model.transform_func)\n",
    "\n",
    "if create_new_tasks:\n",
    "    for i in range(num_tasks):\n",
    "        task_class_indices = sorted(random.sample(class_idxs, k=num_task_classes))\n",
    "\n",
    "        global_to_local = {global_idx: local_idx for local_idx, global_idx in enumerate(task_class_indices)}\n",
    "\n",
    "        task_classes = [class_names[i] for i in task_class_indices]\n",
    "        task_str = \"_\".join([str(i) for i in task_class_indices])\n",
    "\n",
    "        task_dataset = dataset.filter(lambda row: row[\"y\"] in task_class_indices)\n",
    "        task_dataset = task_dataset.map(lambda row: {\"y\": global_to_local[row[\"y\"]]})\n",
    "\n",
    "        task_dataset = task_dataset.map(\n",
    "            desc=f\"Preprocessing samples\",\n",
    "            function=lambda x: {\"x\": transform_func(x[\"x\"])},\n",
    "        )\n",
    "        task_dataset.set_format(type=\"torch\", columns=[\"x\", \"y\", \"id\"])\n",
    "\n",
    "        embeds = MyDatasetDict(train=DatasetDict(), test=DatasetDict())\n",
    "\n",
    "        task = Task(\n",
    "            class_idxs=task_class_indices,\n",
    "            classes=task_classes,\n",
    "            global_to_local=global_to_local,\n",
    "            id=task_str,\n",
    "            embedded_dataset=embeds,\n",
    "            dataset=task_dataset,\n",
    "        )\n",
    "\n",
    "        tasks.append(task)\n",
    "\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from la.utils.utils import get_checkpoint_callback\n",
    "\n",
    "SUBTASK_DATASET_PATH = PROJECT_ROOT / \"matrioska_learning\" / \"subtasks\"\n",
    "\n",
    "\n",
    "if create_new_tasks:\n",
    "    for task in tasks:\n",
    "\n",
    "        print(f\"Training model for task {task.id}\")\n",
    "\n",
    "        num_classes = len(task.classes)\n",
    "        model: pl.LightningModule = hydra.utils.instantiate(\n",
    "            cfg.nn.model,\n",
    "            _recursive_=False,\n",
    "            num_classes=num_classes,\n",
    "            model=cfg.nn.model.model,\n",
    "            input_dim=img_size,\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            task.dataset[\"train\"],\n",
    "            batch_size=100,\n",
    "            pin_memory=False,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            task.dataset[\"test\"],\n",
    "            batch_size=100,\n",
    "            pin_memory=False,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "        template_core: NNTemplateCore = NNTemplateCore(\n",
    "            restore_cfg=cfg.train.get(\"restore\", None),\n",
    "        )\n",
    "        callbacks: List[Callback] = build_callbacks(cfg.train.callbacks, template_core)\n",
    "\n",
    "        storage_dir: str = cfg.core.storage_dir\n",
    "        logger: NNLogger = NNLogger(logging_cfg=cfg.train.logging, cfg=cfg, resume_id=template_core.resume_id)\n",
    "\n",
    "        # Use this in case we need to restore models, search for it in the wandb UI\n",
    "        logger.experiment.config[\"task_classes\"] = task.id\n",
    "\n",
    "        cfg.train.trainer.max_epochs = 1\n",
    "        trainer = pl.Trainer(\n",
    "            default_root_dir=storage_dir,\n",
    "            plugins=[NNCheckpointIO(jailing_dir=logger.run_dir)],\n",
    "            logger=logger,\n",
    "            callbacks=callbacks,\n",
    "            **cfg.train.trainer,\n",
    "        )\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        best_model_path = get_checkpoint_callback(callbacks).best_model_path\n",
    "\n",
    "        task.model = {\n",
    "            \"path\": best_model_path,\n",
    "            \"class\": str(model.__class__.__module__ + \".\" + model.__class__.__qualname__),\n",
    "        }\n",
    "\n",
    "        logger.experiment.finish()\n",
    "\n",
    "        task_dataset = MyDatasetDict(task.dataset)\n",
    "        task_dataset[\"metadata\"] = task.metadata()\n",
    "\n",
    "        Path(f\"{SUBTASK_DATASET_PATH}/{task.id}\").mkdir(exist_ok=True, parents=True)\n",
    "        task_dataset.save_to_disk(f\"{SUBTASK_DATASET_PATH}/{task.id}\")\n",
    "\n",
    "        task_dataset = MyDatasetDict.load_from_disk(f\"{SUBTASK_DATASET_PATH}/{task.id}\")\n",
    "        task.dataset = task_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydoc import locate\n",
    "from nn_core.serialization import load_model\n",
    "\n",
    "\n",
    "SUBTASK_EMBEDDING_PATH = PROJECT_ROOT / \"matrioska_learning\" / \"subtasks_embeddings\"\n",
    "\n",
    "\n",
    "def embed_and_save_samples(task, batch_size=100) -> Dict:\n",
    "    modes = [\"train\", \"test\"]\n",
    "\n",
    "    model_path = task.model[\"path\"]\n",
    "    model_class = locate(task.model[\"class\"])\n",
    "\n",
    "    model = load_model(model_class, checkpoint_path=Path(model_path + \".zip\"))\n",
    "    model.eval().cuda()\n",
    "\n",
    "    for mode in modes:\n",
    "        mode_embeddings = []\n",
    "        mode_ids = []\n",
    "        mode_labels = []\n",
    "\n",
    "        mode_loader = DataLoader(\n",
    "            dataset[mode],\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=True,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "        for batch in tqdm(mode_loader, desc=f\"Embedding {mode} samples for task {task.id}\"):\n",
    "            x = batch[\"x\"].to(\"cuda\")\n",
    "            embeds = model(x)[\"embeds\"].detach()\n",
    "\n",
    "            mode_embeddings.extend(embeds)\n",
    "            mode_ids.extend(batch[\"id\"])\n",
    "            mode_labels.extend(batch[\"y\"])\n",
    "\n",
    "        task.embedded_dataset[mode] = Dataset.from_dict(\n",
    "            {\n",
    "                \"embeds\": mode_embeddings,\n",
    "                \"id\": mode_ids,\n",
    "                \"y\": mode_labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    model.cpu()\n",
    "    task.embedded_dataset.metadata = task.metadata()\n",
    "\n",
    "    (SUBTASK_EMBEDDING_PATH / task.id).mkdir(exist_ok=True, parents=True)\n",
    "    task.embedded_dataset.save_to_disk(SUBTASK_EMBEDDING_PATH / task.id)\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "    embed_and_save_samples(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    print(task.model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space analysis via classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.model_logging import NNLogger\n",
    "from nn_core.serialization import NNCheckpointIO\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Callback\n",
    "from la.pl_modules.classifier import Classifier\n",
    "\n",
    "from la.utils.utils import build_callbacks\n",
    "\n",
    "performance = []\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "    embeds_dataset = task.embedded_dataset\n",
    "    embeds_dataset.set_format(type=\"torch\", columns=[\"embeds\", \"y\"])\n",
    "\n",
    "    embeds_dataset = embeds_dataset.filter(lambda row: row[\"y\"] in task.class_idxs)\n",
    "\n",
    "    eval_train_loader = DataLoader(\n",
    "        embeds_dataset[\"train\"],\n",
    "        batch_size=64,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    eval_test_loader = DataLoader(\n",
    "        embeds_dataset[\"test\"],\n",
    "        batch_size=64,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    model = Classifier(\n",
    "        input_dim=embeds_dataset[\"train\"][\"embeds\"].size(1),\n",
    "        num_classes=len(task.classes),\n",
    "        lr=1e-4,\n",
    "        deep=True,\n",
    "        x_feature=\"embeds\",\n",
    "        y_feature=\"y\",\n",
    "    )\n",
    "\n",
    "    callbacks: List[Callback] = build_callbacks(cfg.train.callbacks)\n",
    "\n",
    "    storage_dir: str = cfg.core.storage_dir\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=storage_dir,\n",
    "        logger=None,\n",
    "        fast_dev_run=False,\n",
    "        gpus=1,\n",
    "        precision=32,\n",
    "        max_epochs=250,\n",
    "        accumulate_grad_batches=1,\n",
    "        num_sanity_val_steps=2,\n",
    "        gradient_clip_val=10.0,\n",
    "        val_check_interval=5.0,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=eval_train_loader, val_dataloaders=eval_test_loader)\n",
    "\n",
    "    classifier_model = trainer.model.eval().cpu().requires_grad_(False)\n",
    "    run_results = trainer.test(model=classifier_model, dataloaders=eval_test_loader)[0]\n",
    "\n",
    "    performance.extend(\n",
    "        (\n",
    "            Result(\n",
    "                matrioska_idx=task.id,\n",
    "                num_train_classes=len(task.classes),\n",
    "                metric_name=\"test_accuracy\",\n",
    "                score=run_results[\"accuracy\"],\n",
    "            ),\n",
    "            Result(\n",
    "                matrioska_idx=task.id,\n",
    "                num_train_classes=len(task.classes),\n",
    "                metric_name=\"test_f1\",\n",
    "                score=run_results[\"f1\"],\n",
    "            ),\n",
    "            Result(\n",
    "                matrioska_idx=task.id,\n",
    "                num_train_classes=len(task.classes),\n",
    "                metric_name=\"test_loss\",\n",
    "                score=run_results[\"test_loss\"],\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
